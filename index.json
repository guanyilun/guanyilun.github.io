[{"content":"Day 1 This question is about finding the first and last \u0026ldquo;digit\u0026rdquo; which could be written as number or word and computing the sum over all input lines. My attempt with julia:\ndigits_lookup = Dict( \u0026#34;0\u0026#34; =\u0026gt; 0, \u0026#34;1\u0026#34; =\u0026gt; 1, \u0026#34;2\u0026#34; =\u0026gt; 2, \u0026#34;3\u0026#34; =\u0026gt; 3, \u0026#34;4\u0026#34; =\u0026gt; 4, \u0026#34;5\u0026#34; =\u0026gt; 5, \u0026#34;6\u0026#34; =\u0026gt; 6, \u0026#34;7\u0026#34; =\u0026gt; 7, \u0026#34;8\u0026#34; =\u0026gt; 8, \u0026#34;9\u0026#34; =\u0026gt; 9, \u0026#34;zero\u0026#34; =\u0026gt; 0, \u0026#34;one\u0026#34; =\u0026gt; 1, \u0026#34;two\u0026#34; =\u0026gt; 2, \u0026#34;three\u0026#34; =\u0026gt; 3, \u0026#34;four\u0026#34; =\u0026gt; 4, \u0026#34;five\u0026#34; =\u0026gt; 5, \u0026#34;six\u0026#34; =\u0026gt; 6, \u0026#34;seven\u0026#34; =\u0026gt; 7, \u0026#34;eight\u0026#34; =\u0026gt; 8, \u0026#34;nine\u0026#34; =\u0026gt; 9 ) function parse_number(line) out, digit = 0, 0 for i in eachindex(line) for (k, v) in digits_lookup if line[i:min(i+length(k)-1, length(line))] == k digit = v break end end out = out \u0026gt; 0 ? out : digit * 10 end out + digit end lines = readlines(\u0026#34;input.txt\u0026#34;) parse_number.(lines) |\u0026gt; sum Day 2 This question practices string splitting. I made use of a trick that each occurance of \u0026ldquo;red\u0026rdquo;, \u0026ldquo;blue\u0026rdquo;, and \u0026ldquo;green\u0026rdquo; in a line corresponds to a different draw, so one can easily use one regular expression to find all matches and then find the upper limit to the number of balls.\nPart 1:\nlines = readlines(\u0026#34;input.txt\u0026#34;) function process_line(line) r, g, b = map(color -\u0026gt; max(map(m -\u0026gt; parse(Int, m.captures[1]), eachmatch(Regex(\u0026#34;(\\\\d+) $(color)\u0026#34;), line))...), [\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;]) (r \u0026lt;= 12 \u0026amp;\u0026amp; g \u0026lt;= 13 \u0026amp;\u0026amp; b \u0026lt;= 14) end map(line -\u0026gt; process_line(line[2]) ? line[1] : 0, enumerate(lines)) |\u0026gt; sum Part 2\nfunction process_line(line) r, g, b = map(color -\u0026gt; max(map(m -\u0026gt; parse(Int, m.captures[1]), eachmatch(Regex(\u0026#34;(\\\\d+) $(color)\u0026#34;), line))...), [\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;]) r * g * b end map(process_line, lines) |\u0026gt; sum Day 3 Day 3 problem is about finding numbers that are next to special characters including diagonally, with an input like\n467..114.. ...*...... ..35..633. ......#... 617*...... .....+.58. ..592..... ......755. ...$.*.... .664.598.. The adjacent criteria can be described with\n\u0026#34;\u0026#34;\u0026#34; x = (line_num, [indices...]) y = (line_num, index) \u0026#34;\u0026#34;\u0026#34; function close_by(x, y) (lx, idx), (ly, idy) = x, y (abs(lx - ly) \u0026gt; 1) \u0026amp;\u0026amp; return false ((idy \u0026gt; maximum(idx) + 1) || (idy \u0026lt; minimum(idx) - 1)) \u0026amp;\u0026amp; return false return true end where x is a tuple that contains the line number and the string indices of each number, and y is a tuple that similarly contains the line number and an index of the special character (always has length 1).\nNext we can parse the numbers and special characters in the format expected by this function with\nspc = []; num = [] map(x -\u0026gt; map(m-\u0026gt;push!(spc, (x[1], m[1])), findall(r\u0026#34;[^0-9\\.]\u0026#34;, x[2])), enumerate(lines)) map(x -\u0026gt; map(m-\u0026gt;push!(num, (x[1], collect(m))), findall(r\u0026#34;(\\d+)\u0026#34;, x[2])), enumerate(lines)) where we have used the regex exclusion syntax [^0-9.] to exclude all dots and numbers to help us find special characters, and have used matched group notation (\\d+) to find all numbers in a line.\nThe question asks us for the sum of all numbers that are adjacent to a special character, it can be simply calculated with\nsum(num |\u0026gt; filter(x -\u0026gt; any(map(y -\u0026gt; close_by(x, y), spc))) .|\u0026gt; x -\u0026gt; parse(Int, lines[x[1]][x[2]])) For part 2, it defines a concept of \u0026ldquo;gear\u0026rdquo; which is a \u0026ldquo;*\u0026rdquo; character that has exactly two numbers adjacent to it. And, the question asks us to find all gears in an input text and compute the product of two numbers for each gear, and then sum the result for all gears. This can be easily achieved with\nprod = 0 for c in spc if lines[c[1]][c[2]] == \u0026#39;*\u0026#39; nums_close = num |\u0026gt; filter(n-\u0026gt;close_by(n, c)) (length(nums_close) == 2) \u0026amp;\u0026amp; (prod += mapreduce(n-\u0026gt;parse(Int, lines[n[1]][n[2]]), *, nums_close)) end end prod or, for people who like one-liner,\n((spc |\u0026gt; filter(x -\u0026gt; lines[x[1]][x[2]] == \u0026#39;*\u0026#39;)) .|\u0026gt; c -\u0026gt; ((num |\u0026gt; filter(n-\u0026gt;close_by(n, c))) |\u0026gt; length) == 2 ? mapreduce(n-\u0026gt;parse(Int, lines[n[1]][n[2]]), *, num |\u0026gt; filter(n-\u0026gt;close_by(n, c))) : 0) |\u0026gt; sum Day 4 Today\u0026rsquo;s question is about finding matching numbers on the card,\nCard 1: 41 48 83 86 17 | 83 86 6 31 17 9 48 53 Card 2: 13 32 20 16 61 | 61 30 68 82 17 32 24 19 Card 3: 1 21 53 59 44 | 69 82 63 72 16 21 14 1 Card 4: 41 92 73 84 69 | 59 84 76 51 58 5 54 83 Card 5: 87 83 26 28 32 | 88 30 70 12 93 22 82 36 Card 6: 31 18 13 56 72 | 74 77 10 23 35 67 36 11 For example, in the first card, there are four matching numbers.\nThe first part of the question asks us to compute the score of the card which is calculated as the following: first matching number on a card gives a score of 1, any additional matches doubles the existing score. The question requires us to calculate the total score given an input list of cards.\nOne simple way to find common numbers amount list is through Set interaction:\n\u0026#34;\u0026#34;\u0026#34; expr: string with numbers separated by spaces \u0026#34;\u0026#34;\u0026#34; function parse_int(expr) strip(expr) |\u0026gt; x -\u0026gt; split(x, r\u0026#34;\\s+\u0026#34;) .|\u0026gt; x -\u0026gt; parse(Int, x) end \u0026#34;\u0026#34;\u0026#34; count the number of winning numbers \u0026#34;\u0026#34;\u0026#34; function get_match_count(line) (split(line, \u0026#34;:\u0026#34;)[2] |\u0026gt; x -\u0026gt; split(x, \u0026#34;|\u0026#34;) .|\u0026gt; parse_int .|\u0026gt; Set) |\u0026gt; x -\u0026gt; intersect(x...) |\u0026gt; length end # get total score (get_match_count.(lines) .|\u0026gt; x -\u0026gt; x == 0 ? 0 : 2^(x-1)) |\u0026gt; sum where we defined the get_match_count function to find the total number of winning numbers based on the number of elements in the intersection between the two lists. We then use the numbers for each card to calculate the total score based on the description, which is \\(2^{x-1}\\) except when \\(x\\) is 0 which has a zero score.\nFor part two, instead of calculating scores, each card rewards subsequent n cards with n the number of matching numbers in the card. For example, card 1 has 4 matching numbers and will give card 2, 3, 4, 5 as reward, so now there are two copies of card 2 for example. When resolving card 2 award, we repeat the process except we have two copies of card 2 now. It accumulates throughout the card stack, and the question asks us to find the total number of cards at the end of resolving all cards. Here is my solution:\nmultiplier = [ones(length(lines)); zeros(10)] for (i, line) in enumerate(lines) n_match = get_match_count(line) multiplier[i+1:i+n_match] .+= multiplier[i] end sum(multiplier) where I used the multiplier to keep track of the number of card copies. The 10 additional zeros I padded to the multiplier is to ensure the multiplier[i+i:n_match] never goes out of range.\nDay 5 Today\u0026rsquo;s question tests table mapping. It defines some mapping rules from various quantities such as mapping between seed and soil, mapping between soil to fertilizer, and so on, eventually to location. The rule is given like this\nseeds: 79 14 55 13 seed-to-soil map: 50 98 2 52 50 48 soil-to-fertilizer map: 0 15 37 37 52 2 39 0 15 fertilizer-to-water map: 49 53 8 0 11 42 42 0 7 57 7 4 water-to-light map: 88 18 7 18 25 70 light-to-temperature map: 45 77 23 81 45 19 68 64 13 temperature-to-humidity map: 0 69 1 1 0 69 humidity-to-location map: 60 56 37 56 93 4 For part one, the question asked us to traverse the series of mapping to find the correct location for each input seed and identifies the nearest location which corresponds to then smallest location id.\nFirst we need to work on parsing the input text:\nseeds = match(r\u0026#34;seeds: (.*)\\n\u0026#34;, text).captures[1] |\u0026gt; split .|\u0026gt; x -\u0026gt; parse(Int, x) parse_arr = cat -\u0026gt; match(Regex(\u0026#34;$(cat) map:\\\\s*\\\\n((?:\\\\s*\\\\d+\\\\s+\\\\d+\\\\s+\\\\d+\\\\s*\\\\n)+)\u0026#34;), text).captures[1] |\u0026gt; split |\u0026gt; x -\u0026gt; reshape(x, 3, :) .|\u0026gt; x -\u0026gt; parse.(Int, x) parse_map = cat -\u0026gt; parse_arr(cat) |\u0026gt; x -\u0026gt; map(eachcol(x)) do m ((m[2], m[2]+m[3]-1), (m[1], m[1]+m[3]-1)) end mappings = parse_map.([ \u0026#34;seed-to-soil\u0026#34;, \u0026#34;soil-to-fertilizer\u0026#34;, \u0026#34;fertilizer-to-water\u0026#34;, \u0026#34;water-to-light\u0026#34;, \u0026#34;light-to-temperature\u0026#34;, \u0026#34;temperature-to-humidity\u0026#34;, \u0026#34;humidity-to-location\u0026#34; ]) In the input file, the mapping rule is specified as (dest_start, src_start, range); I have converted it to a more readable format: ((src_start, src_stop), (dest_start, dest_stop)) with the parse_map function. The game rule also specifies that any numbers not covered in the specified ranges will passthrough to itself. For completeness, I will also add these dummy ranges: ((0, first), (0, first)) and ((last, inf), (last, inf)). I used inf because I don\u0026rsquo;t know the largest number in the input text and I don\u0026rsquo;t want to compute it unnecessarily:\nmappings = map(mappings) do mapping strt = (0, minimum(mapping[i][1][1] for i in eachindex(mapping))) stop = (maximum(mapping[i][1][2] for i in eachindex(mapping)), Inf) strt[2] == 0 ? (mapping..., (stop, stop)) : ((strt, strt), mapping..., (stop, stop)) end To apply a mapping, we simply go through the source ranges to find a match and apply the right offset to the destination range.\nfunction apply_mapping(i, mapping) for m in mapping (m[1][1] \u0026lt;= i \u0026lt;= m[1][2]) \u0026amp;\u0026amp; return m[2][1] + (i - m[1][1]) end return i end Then we can find the \u0026ldquo;location\u0026rdquo; for each \u0026ldquo;seed\u0026rdquo; by traversing all mappings via\nfoldl((x, f) -\u0026gt; f.(x), [x -\u0026gt; apply_mapping(x, m) for m in mappings], init=seeds) |\u0026gt; minimum and get the minimum location. This solves the first part.\nIn part 2 of the question, it suggests the input seeds, instead of being individual seeds, are actually pairs of numbers, with the second number specifying a range. The method that we setup in part 1 then becomes very costly because it may require us to repeatedly calculating mapping over up to 1e9 points (as in the input). It\u0026rsquo;s not prohibitive for a modern computer but unnecessary, because we could simply keep track of how intervals map to each other, i.e., how different intervals in \u0026ldquo;seeds\u0026rdquo; space map onto \u0026ldquo;locations\u0026rdquo; space. To work out the interval mapping, we first setup some useful functions that deals with intersections of intervals\nhas_intersect = (inter1, inter2) -\u0026gt; sign(inter1[2] - inter2[1]) * sign(inter1[1] - inter2[2]) \u0026lt;= 0 interval_intersect = (inter1, inter2) -\u0026gt; begin !has_intersect(inter1, inter2) \u0026amp;\u0026amp; return nothing (max(inter1[1], inter2[1]), min(inter1[2], inter2[2])) end To map intervals in input space of map1 to output space of map2, we can use this function\nfunction reduce_mapping(map1, map2) omap = [] for m1 in map1 for m2 in map2 inter = interval_intersect(m1[2], m2[1]) if !isnothing(inter) offset1 = m1[1][1] - m1[2][1] offset2 = -m2[1][1] + m2[2][1] push!(omap, (inter .+ offset1, inter .+ offset2)) end end end omap end For example, given\nmap1 = (((0, 50), (0, 50)), ((98, 99), (50, 51)), ((50, 97), (52, 99)), ((99, Inf), (99, Inf))) map2 = (((15, 51), (0, 36)), ((52, 53), (37, 38)), ((0, 14), (39, 53)), ((53, Inf), (53, Inf))) the reduced mapping is\nreduce_mapping(map1, map2) = (((15, 50), (0, 35)), ((0, 14), (39, 53)), ((98, 99), (35, 36)), ((50, 51), (37, 38)), ((51, 97.0), (53, 99.0)), ((99, Inf), (99, Inf))) We can then apply this reduction through all mappings to work out interval mappings between seed space and location space, by\nomap = foldl((x, y) -\u0026gt; reduce_mapping(x, y), mappings) To find out the nearest location again like in part one, we can find intersections of the input ranges to the intervals specified in the mapping and figure out how these intervals mapped to location space to identify the nearest location. The math is identical to how we compute the combined mapping in reduce_mapping, so we can reuse it with\n# now seeds are pairs of numbers which we will convert to a mapping format imap = seeds |\u0026gt; x -\u0026gt; reshape(x, 2, :) |\u0026gt; x -\u0026gt; map(eachcol(x)) do c ((c[1], c[1] + c[2] - 1), (c[1], c[1] + c[2] - 1)) end reduce_mapping(imap, omap) |\u0026gt; m -\u0026gt; map(x -\u0026gt; x[2][1], m) |\u0026gt; minimum This allows me to easily find the minimum location.\n","permalink":"https://guanyilun.github.io/posts/advent_of_code/","summary":"Day 1 This question is about finding the first and last \u0026ldquo;digit\u0026rdquo; which could be written as number or word and computing the sum over all input lines. My attempt with julia:\ndigits_lookup = Dict( \u0026#34;0\u0026#34; =\u0026gt; 0, \u0026#34;1\u0026#34; =\u0026gt; 1, \u0026#34;2\u0026#34; =\u0026gt; 2, \u0026#34;3\u0026#34; =\u0026gt; 3, \u0026#34;4\u0026#34; =\u0026gt; 4, \u0026#34;5\u0026#34; =\u0026gt; 5, \u0026#34;6\u0026#34; =\u0026gt; 6, \u0026#34;7\u0026#34; =\u0026gt; 7, \u0026#34;8\u0026#34; =\u0026gt; 8, \u0026#34;9\u0026#34; =\u0026gt; 9, \u0026#34;zero\u0026#34; =\u0026gt; 0, \u0026#34;one\u0026#34; =\u0026gt; 1, \u0026#34;two\u0026#34; =\u0026gt; 2, \u0026#34;three\u0026#34; =\u0026gt; 3, \u0026#34;four\u0026#34; =\u0026gt; 4, \u0026#34;five\u0026#34; =\u0026gt; 5, \u0026#34;six\u0026#34; =\u0026gt; 6, \u0026#34;seven\u0026#34; =\u0026gt; 7, \u0026#34;eight\u0026#34; =\u0026gt; 8, \u0026#34;nine\u0026#34; =\u0026gt; 9 ) function parse_number(line) out, digit = 0, 0 for i in eachindex(line) for (k, v) in digits_lookup if line[i:min(i+length(k)-1, length(line))] == k digit = v break end end out = out \u0026gt; 0 ?","title":"Advent of Code"},{"content":"Gauge in Cosmology (2023/11/14) Why is there gauge freedom when analyzing cosmological perturbations? I think this lies in the fact that our background universe is homogeneous and isotropic, and density perturbations are perturbatively small. An observer in FLRW effectively experiences no gravity in the background level and thus has no preferred inertial frame background universe. The observer thus has the freedom to choose an inertial frame, as long as it doesn\u0026rsquo;t violate the perturbation limit, to describe his observable universe, and this introduces the gauge freedom.\n","permalink":"https://guanyilun.github.io/posts/shower/","summary":"Gauge in Cosmology (2023/11/14) Why is there gauge freedom when analyzing cosmological perturbations? I think this lies in the fact that our background universe is homogeneous and isotropic, and density perturbations are perturbatively small. An observer in FLRW effectively experiences no gravity in the background level and thus has no preferred inertial frame background universe. The observer thus has the freedom to choose an inertial frame, as long as it doesn\u0026rsquo;t violate the perturbation limit, to describe his observable universe, and this introduces the gauge freedom.","title":"Shower Thoughts"},{"content":"A repository to keep track of random little things I learned or thought about each day.\nThe Cosmic Dipole Problem (2023/11/29) CMB features a dipole anisotropy which is interpreted as our motion relative to the cosmic rest frame. If this interpretation is correct, one expects other probe of this relative motion to feature the same cosmic dipole. However measurements from radio galaxies and quasars have presented some discrepancies in the magnitude of the cosmic dipole (see, e.g., (Peebles 2022; Kumar Aluri et al. 2023) for reviews).\nA new analysis by (Mittal, Oayda, and Lewis 2023) has argued otherwise. They used 1.3 million quasars from the recently-released Quaia quasar catalogue and found that the previous discrepancies are likely due to contaminations near the Galactic Plane, and as they exclude the contaminated samples, they arrive at a result consistent both in direction and magnitude with the CMB dipole.\nObservation matrix (2023/11/13) Observation matrix, \\(M\\), is defined as\n\\begin{equation} m_{\\rm out} = M m_{\\rm in}. \\end{equation}\nThe sky signal observed by detectors, \\(s\\), is given by\n\\begin{equation} s = Pm_{\\rm in}, \\end{equation}\nwhere \\(P\\) is the pointing matrix. It can be solved by\n\\begin{equation} m = (P^{T}N^{-1}P)^{-1}P^TN^{-1}s \\equiv Bs, \\end{equation}\nwhere \\(N\\) is the noise covariance matrix. One may want to apply additional filtering to deproject a set of time-domain templates, given as columns in \\(F\\) with\n\\begin{equation} Z = 1 - F(F^TN^{-1}F)F^TN^{-1}, \\end{equation}\nwhich represents deprojecting best-fit match to given set of templates. As a result,\n\\begin{equation} m_{\\rm out} = BZs = BZPm_{\\rm in}. \\end{equation}\nTherefore, the observation matrix is given by\n\\begin{equation} M = BZP = (P^{T}N^{-1}P)^{-1}P^TN^{-1}(1 - F(F^TN^{-1}F)F^TN^{-1})P. \\end{equation}\nBlackhole Superradiance (2023/11/13) Superradiance is a phenomenon occurring when a wave interacts with a rotating object that has dissipative or absorptive properties. This interaction can cause the incoming wave to gain energy. For example, in a classical scenario, if a ball strikes a rotating cylinder, the ball can gain energy if the cylinder’s rotation speed exceeds the ball’s speed. Conversely, the ball loses energy if the cylinder rotates more slowly.\nIn astrophysics, blackhole is a perfect absorber, and a rotating blackhole, i.e., Kerr blackhole, will exhibit superradiance. If the incoming wave comes from a massless scalar field, it will have its energy boosted similar to the example of the ball. If the incoming wave comes from a massive scalar field, on the other hand, the massive field will be bound by the gravitational potential of the blackhole and form a bound state very much like a hydrogen atom. When a wave gets amplified by the blackhole through superradiance, it remains trapped in the bounded state and will keep getting amplified; this leads to a runaway growth of a cloud of particles around the blackhole, and as a result, the blackhole spin gets slowed down. As the dynamical time for the superradiance is much shorter than the lifetime of the blackhole, it will have significant impact on the evolution of the blackhole. Superradiance with massive particles therefore predicts a specific population of blackhole spins (with more low spin than high spin blackholes); this can be tested with observations from gravitational wave observatories such as LIGO.\nSource: https://youtu.be/DJPFPs2kDxM?si=vgtjAIvb8EhHC6HP\nSymbolic Tracing (2023/01/25) After spending several hours reading jax\u0026rsquo;s documentation. I have finally grasped the basic idea of it. I will try to summarize it in words. It is built upon an elegant concept which I call function reinterpretation. The idea is that, given an unknown implementation of a function, one can define a transformation that reinterprets the implementation of the function without knowing its details as long as the underlying function is composable of a well-defined set of primitive functions that we have provided. It sounds like magic, but it is indeed feasible and lies at the heart of jax.\njax implements this by passing in a special object known as the Tracer to the function to obtain a specific functional Trace of our target. Intuitively, this is like attaching a mini camera to our input variable so it can observe the system and trace out its inner workings.\nTo achieve this, first we need to design our primitives such that their evaluation can be intercepted and routed to our target Trace object which encapsulates the specific function reinterpretation that we want to do. We then define a corresponding Tracer object that can be used to perform (or realize) the Trace transformation. Tracer also carries important data relevant to the specific tracing analysis. When we pass these tracers into the function we are probing, our tracers will trigger a series of primitive function calls which are then routed to our Trace object, which defines how to reinterpret each specific primitive function in terms of custom operations on the \u0026ldquo;payload\u0026rdquo; of our Tracer. To give a specific example, suppose our function contains an addition of two variables, such as\ndef target(a, b): c = a + b d = c + a return d and that we want to map all additions inside our target function to multiplications, we can define two Tracer objects that carry payload of a, b, respectively. When our Trace intercepts the calls from \u0026ldquo;addition\u0026rdquo; primitives, it then takes the two tracer arguments, multiplies their payload and promotes it to another Tracer object with a payload of a*b to proceed to the subsequent computations. At the end of the function call, we will get a tracer object as the return. We can then simply read off its payload which represents the result of an reinterpretation of our target function that turns addition into multiplication. If we redefine the entire process as a new function, such as the following pseudocode,\ndef add2mul(func): def f(a, b): return func(Tracer(payload=a), Tracer(payload=b)).payload return f transformed_target = add2mul(target) We have effectively transformed our target function into a different function by modifying its inner computation without actually knowing its implementation details!\nUseful resources Pytorch symbolic tracing paper: https://arxiv.org/pdf/2112.08429.pdf Explanation of jax core: https://jax.readthedocs.io/en/latest/autodidax.html Lightweight moduler staging: https://web.stanford.edu/class/cs442/lectures_unrestricted/cs442-lms.pdf References Kumar Aluri, Pavan, Paolo Cea, Pravabati Chingangbam, Ming-Chung Chu, Roger G. Clowes, Damien Hutsemékers, Joby P. Kochappan, et al. 2023. “Is the observable Universe consistent with the cosmological principle?” Classical and Quantum Gravity 40 (9): 094001. https://doi.org/10.1088/1361-6382/acbefc. Mittal, Vasudev, Oliver T. Oayda, and Geraint F. Lewis. 2023. “The Cosmic Dipole in the Quaia Sample of Quasars: A Bayesian Analysis.” Arxiv E-Prints, November, arXiv:2311.14938. https://doi.org/10.48550/arXiv.2311.14938. Peebles, P. J. E. 2022. “Anomalies in physical cosmology.” Annals of Physics 447 (December): 169159. https://doi.org/10.1016/j.aop.2022.169159. ","permalink":"https://guanyilun.github.io/posts/til/","summary":"A repository to keep track of random little things I learned or thought about each day.\nThe Cosmic Dipole Problem (2023/11/29) CMB features a dipole anisotropy which is interpreted as our motion relative to the cosmic rest frame. If this interpretation is correct, one expects other probe of this relative motion to feature the same cosmic dipole. However measurements from radio galaxies and quasars have presented some discrepancies in the magnitude of the cosmic dipole (see, e.","title":"Today I Learned"},{"content":"Isotropic Methods Minami et al. 2019: describes a method to simutaneously determine birefringence and detector miscalibration using galactic foreground emission, assuming the galactic foreground has zero EB correlation. Sherwin and Namikawa 2021: miscalibration free measurements of isotropic birefringence using reionization bump. Namikawa 2021: use CMB mode coupling to constrain isotropic birefringence. Lee, Hotinli, and Kamionkowski 2022: use polarized SZ effect to probe cosmic birefringence. Diego-Palazuelos, Mart\\’ınez-González, et al. 2022: validate method of (Minami et al. 2019) with realistic simulations of Planck HFI. Conclude that the method is robust to miscalibration error but suffer from foreground EB. Jost, Errard, and Stompor 2022: proposes a generalized parametric foreground separation approach based on CMB polarization that accounts for cosmic birefringence. Claims that 0.35\\(^{\\circ}\\) detection can be ruled out at 5\\(\\sigma\\) by near future CMB experiment using this approach, assuming an in-lab calibration priors. Measurements Minami and Komatsu 2020: performs a search based on method described in (Minami et al. 2019), finding \\(\\beta=0.35\\pm 0.14^{\\circ}\\). Diego-Palazuelos, Eskilt, et al. 2022: repeat (Minami and Komatsu 2020) with Planck PR4 data, finding \\(\\beta=0.30\\pm 0.11^{\\circ}\\). Ferguson et al. 2022: measure time-dependent rotation with SPT-3G data. Eskilt and Komatsu 2022: measure isotropic birefringence using Planck and WMAP, arriving at \\(\\beta=0.342 \\pm 0.09^{\\circ}\\) which has a statistical significance of 3.6\\(\\sigma\\). Anistropic Measurements Namikawa et al. 2020: constrain anistropic birefringence using quadratic estimator with ACT DR4 data, finding an upper limit of \\(A_{\\rm CB}=10^{-5}\\). Bianchini et al. 2020: constrain with SPTpol data, finding consistent upperlimit as (Namikawa et al. 2020). Systematics Planck on dust polarization: Planck 353GHz data shows a EE/BB ratio of about 2, with a positive TE and a weaker, parity violating, TB have been detected, while dust EB remains compatible with 0. Clark et al. 2021: investigate origin of EB correlation in Galactic foreground, suggesting its a result of misalignment between filamentary structure and magnetic field. Cukierman, Clark, and Halal 2022: provides observational evidence for misalignment between filaments and magnetic field by comparing Planck polarized dust emission and HI measurement from HI4PI. They find a \\(\\sim 2^{\\circ}\\) global misalignment which is roughly scale independent. Vacher et al. 2022: extending Clark et al. 2021 and Cukierman, Clark, and Halal 2022. Developed an analytic framework and worked out the frequency dependence of dust spectrum such as EE, BB, and EB. Monelli et al. 2022: study non-idealities of half-wave plate as a source of cosmic birefringence systematics. Physics Nakatsuka, Namikawa, and Komatsu 2022: calculate EB power spectrum from axion model, following similar idea as (Sherwin and Namikawa 2021) which uses the difference between recombination and reionization. Focused on isotropic birefringence. Greco, Bartolo, and Gruppuso 2022: extends Nakatsuka, Namikawa, and Komatsu 2022 to anistropic birefringence. Kitajima et al. 2022: explains birefringence through domain walls of axion-like particles Jain et al. 2022: calculates the expected isotropic and anisotropic birefringence from a network of cosmic string and domain walls formed by axion-like particles. They constrained such effect using existing anistropic measurements. They also found that the isotropic birefringence detected and the non-detection of anistropic birefringence signal is incompatible within this model. Tools Cai and Guan 2021: modified class to calculate rotated power spectrum using non-perturbative calculation. It supports both isotropic and anisotropic birefringences. References Bianchini, F., W. L. K. Wu, P. A. R. Ade, A. J. Anderson, J. E. Austermann, J. S. Avva, L. Balkenhol, et al. 2020. “Searching for anisotropic cosmic birefringence with polarization data from SPTpol” 102 (8): 083504. https://doi.org/10.1103/PhysRevD.102.083504. Cai, Hongbo, and Yilun Guan. 2021. “Computing Microwave Background Polarization Power Spectra from Cosmic Birefringence.” Arxiv E-Prints, November, arXiv:2111.14199. Clark, S. E., Chang-Goo Kim, J. Colin Hill, and Brandon S. Hensley. 2021. “The Origin of Parity Violation in Polarized Dust Emission and Implications for Cosmic Birefringence” 919 (1): 53. https://doi.org/10.3847/1538-4357/ac0e35. Cukierman, Ari J., S. E. Clark, and George Halal. 2022. “Magnetic Misalignment of Interstellar Dust Filaments.” Arxiv E-Prints, August, arXiv:2208.07382. Diego-Palazuelos, P., J. R. Eskilt, Y. Minami, M. Tristram, R. M. Sullivan, A. J. Banday, R. B. Barreiro, H. K. Eriksen, et al. 2022. “Cosmic Birefringence from Planck Data Release 4.” Arxiv E-Prints, January, arXiv:2201.07682. Diego-Palazuelos, P., E. Mart\\’ınez-González, P. Vielva, R. B. Barreiro, M. Tristram, E. de la Hoz, J. R. Eskilt, Y. Minami, et al. 2022. “Robustness of cosmic birefringence measurement against Galactic foreground emission and instrumental systematics.” Arxiv E-Prints, October, arXiv:2210.07655. Eskilt, Johannes R., and Eiichiro Komatsu. 2022. “Improved constraints on cosmic birefringence from the WMAP and Planck cosmic microwave background polarization data” 106 (6): 063503. https://doi.org/10.1103/PhysRevD.106.063503. Ferguson, K. R., A. J. Anderson, N. Whitehorn, P. A. R. Ade, M. Archipley, J. S. Avva, L. Balkenhol, et al. 2022. “Searching for axionlike time-dependent cosmic birefringence with data from SPT-3G” 106 (4): 042011. https://doi.org/10.1103/PhysRevD.106.042011. Greco, Alessandro, Nicola Bartolo, and Alessandro Gruppuso. 2022. “Probing Axions through Tomography of Anisotropic Cosmic Birefringence.” arXiv. https://arxiv.org/abs/2211.06380. Jain, Mudit, Ray Hagimoto, Andrew J. Long, and Mustafa A. Amin. 2022. “Searching for axion-like particles through CMB birefringence from string-wall networks” 2022 (10): 090. https://doi.org/10.1088/1475-7516/2022/10/090. Jost, Baptiste, Josquin Errard, and Radek Stompor. 2022. “Characterising cosmic birefringence in the presence of galactic foregrounds and instrumental systematic effects.” Arxiv E-Prints, December, arXiv:2212.08007. https://doi.org/10.48550/arXiv.2212.08007. Kitajima, Naoya, Fumiaki Kozai, Fuminobu Takahashi, and Wen Yin. 2022. “Power spectrum of domain-wall network, and its implications for isotropic and anisotropic cosmic birefringence” 2022 (10): 043. https://doi.org/10.1088/1475-7516/2022/10/043. Lee, Nanoom, Selim C. Hotinli, and Marc Kamionkowski. 2022. “Probing cosmic birefringence with polarized Sunyaev-Zel’dovich tomography” 106 (8): 083518. https://doi.org/10.1103/PhysRevD.106.083518. Minami, Yuto, Hiroki Ochi, Kiyotomo Ichiki, Nobuhiko Katayama, Eiichiro Komatsu, and Tomotake Matsumura. 2019. “Simultaneous determination of the cosmic birefringence and miscalibrated polarization angles from CMB experiments.” Progress of Theoretical and Experimental Physics 2019 (8): 083E02. https://doi.org/10.1093/ptep/ptz079. Minami, Yuto, and Eiichiro Komatsu. 2020. “New Extraction of the Cosmic Birefringence from the Planck 2018 Polarization Data.” $\\Backslash$prl 125 (22): 221301. https://doi.org/10.1103/PhysRevLett.125.221301. Monelli, Marta, Eiichiro Komatsu, Alexandre E. Adler, Matteo Billi, Paolo Campeti, Nadia Dachlythra, Adriaan J. Duivenvoorden, Jon E. Gudmundsson, and Martin Reinecke. 2022. “Impact of half-wave plate systematics on the measurement of cosmic birefringence from CMB polarization.” Arxiv E-Prints, November, arXiv:2211.05685. https://doi.org/10.48550/arXiv.2211.05685. Nakatsuka, Hiromasa, Toshiya Namikawa, and Eiichiro Komatsu. 2022. “Is cosmic birefringence due to dark energy or dark matter? A tomographic approach.” Arxiv E-Prints, March, arXiv:2203.08560. Namikawa, Toshiya. 2021. “CMB Mode Coupling with Isotropic Polarization Rotation.” Monthly Notices of the Royal Astronomical Society 506 (1): 1250–57. https://doi.org/10.1093/mnras/stab1796. Namikawa, Toshiya, Yilun Guan, Omar Darwish, Blake D. Sherwin, and others. 2020. “The Atacama Cosmology Telescope: Constraints on cosmic birefringence.” Physical Review D 101 (8): 1–18. https://doi.org/10.1103/physrevd.101.083527. Sherwin, Blake D., and Toshiya Namikawa. 2021. “Cosmic Birefringence Tomography and Calibration-Independence with Reionization Signals in the CMB” 7 (August): 1–7. https://arxiv.org/abs/2108.09287. Vacher, Léo, Jonathan Aumont, François Boulanger, Ludovic Montier, Vincent Guillet, Alessia Ritacco, and Jens Chluba. 2022. “Frequency dependence of the thermal dust $E/B$ ratio and $EB$ correlation: insights from the spin-moment expansion.” Arxiv E-Prints, October, arXiv:2210.14768. https://doi.org/10.48550/arXiv.2210.14768. ","permalink":"https://guanyilun.github.io/posts/birefringence/","summary":"Isotropic Methods Minami et al. 2019: describes a method to simutaneously determine birefringence and detector miscalibration using galactic foreground emission, assuming the galactic foreground has zero EB correlation. Sherwin and Namikawa 2021: miscalibration free measurements of isotropic birefringence using reionization bump. Namikawa 2021: use CMB mode coupling to constrain isotropic birefringence. Lee, Hotinli, and Kamionkowski 2022: use polarized SZ effect to probe cosmic birefringence. Diego-Palazuelos, Mart\\’ınez-González, et al. 2022: validate method of (Minami et al.","title":"Cosmic Birefringence"}]